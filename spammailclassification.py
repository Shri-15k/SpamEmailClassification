# -*- coding: utf-8 -*-
"""SpamMailClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JUAcEJsQjxOSWnWfsPxONmfc__uMEYLN
"""

# Install required libraries
!pip install numpy pandas scikit-learn nltk matplotlib

"""**numpy**: For numerical computations.
**pandas**: For handling data in tabular form.
**scikit-learn**: For machine learning models and evaluation metrics.
**nltk**: A powerful library for text preprocessing and NLP tasks.
**matplotlib**: For visualizing the data.

Loading and Understanding the Dataset
"""

import pandas as pd

df = pd.read_csv('spam.csv')

print("Dataset Preview:")
print(df.head())

print("Dataset Info:")
print(df.info())

print("Label Distribution:")
print(df['Category'].value_counts())

"""**Data Preprocessing**

Emails often contain noise (e.g., punctuation, special characters) that does not help in classification. Preprocessing cleans and transforms the text into a structured format suitable for machine learning models.

Rename Columns and Encode Labels
"""

#Rename columns for clarity
df.rename(columns={"Category": "Label", "Message": "Email"}, inplace=True)

#Encode labels: 'ham' - 0, 'spam' - 1
df['Label'] = df['Label'].map({'ham': 0, 'spam': 1})

print(df.head())

"""Check for Missing or Duplicate Data"""

print("Missing Values:")
print(df.isnull().sum())

print("\nDuplicate Emails:")
print(df.duplicated().sum())

df.drop_duplicates(inplace=True)
print("\nAfter removing duplicates, dataset shape:", df.shape)

"""Clean the Email Text"""

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

#Initialize stopwords
stop_words = set(stopwords.words('english'))

#Define a function for text cleaning
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = ' '.join(word for word in text.split() if word not in stop_words)  # Remove stopwords
    return text


df['Cleaned_Email'] = df['Email'].apply(clean_text)

# Preview the cleaned emails
print(df[['Email', 'Cleaned_Email']].head())

"""**Feature Engineering**

Now that we’ve cleaned the email text, we need to convert it into numerical features so that machine learning models can understand it. For this, we’ll use TF-IDF (Term Frequency-Inverse Document Frequency).

Split Data into Train and Test Sets
"""

from sklearn.model_selection import train_test_split

X = df['Cleaned_Email']
y = df['Label']

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set size:", X_train.shape[0])
print("Test set size:", X_test.shape[0])

"""Transform Text Data Using TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

#Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=3000)  # Limit to top 3000 words to reduce dimensionality


X_train_tfidf = tfidf.fit_transform(X_train)

#Transform the test data
X_test_tfidf = tfidf.transform(X_test)

print("TF-IDF feature matrix (training):", X_train_tfidf.shape)

"""This indicates 4457 emails and 3000 features (words).

**Model Training**

Now that the text has been transformed into numerical features using TF-IDF, it’s time to train machine learning models. We'll start with a simple model: Naive Bayes, which is well-suited for text classification tasks.

Train a Naive Bayes Model
"""

from sklearn.naive_bayes import MultinomialNB

nb_model = MultinomialNB()

nb_model.fit(X_train_tfidf, y_train)

print("Naive Bayes model trained successfully!")

"""Evaluate the Model"""

from sklearn.metrics import classification_report, accuracy_score

y_pred = nb_model.predict(X_test_tfidf)

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))

print("Accuracy Score:", accuracy_score(y_test, y_pred))

"""Compare with Other Models

Logistic Regression:
"""

from sklearn.linear_model import LogisticRegression

#Initialize Logistic Regression model
lr_model = LogisticRegression()

lr_model.fit(X_train_tfidf, y_train)

y_pred_lr = lr_model.predict(X_test_tfidf)

print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))

"""**Save the Trained Model**

Save the Model and TF-IDF Vectorizer
"""

import joblib

#Save the Naive Bayes model
joblib.dump(nb_model, 'spam_classifier_model.pkl')

joblib.dump(tfidf, 'tfidf_vectorizer.pkl')

print("Model and vectorizer saved successfully!")

"""Load the Saved Model for Testing"""

#Load the saved model and vectorizer
loaded_model = joblib.load('spam_classifier_model.pkl')
loaded_tfidf = joblib.load('tfidf_vectorizer.pkl')


sample_emails = ["Congratulations! You've won a $1000 gift card.",
                 "Hey, can we meet tomorrow for lunch?"]

sample_emails_tfidf = loaded_tfidf.transform(sample_emails)

predictions = loaded_model.predict(sample_emails_tfidf)

for email, label in zip(sample_emails, predictions):
    print(f"Email: '{email}' - Prediction: {'Spam' if label == 1 else 'Ham'}")